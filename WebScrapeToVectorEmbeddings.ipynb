{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNICeWCg3JdZlihRnPHZHle",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesDRodgers/Course-Projects-Gen-AI/blob/main/WebScrapeToVectorEmbeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q langchain-openai beautifulsoup4 faiss-cpu tqdm\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import faiss\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "from typing import Tuple, List, Optional, Dict\n",
        "import logging\n",
        "from google.colab import files\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class WebsiteEmbeddingCreator:\n",
        "    def __init__(self, api_key: str):\n",
        "        \"\"\"Initialize the embedding creator with OpenAI API key.\"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.embeddings = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            length_function=len\n",
        "        )\n",
        "\n",
        "    def scrape_website(self, url: str) -> Optional[str]:\n",
        "        \"\"\"Scrape and clean text from a website.\"\"\"\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Add retry mechanism\n",
        "            for attempt in range(3):\n",
        "                try:\n",
        "                    response = requests.get(url, headers=headers, timeout=10)\n",
        "                    response.raise_for_status()\n",
        "                    break\n",
        "                except requests.RequestException as e:\n",
        "                    if attempt == 2:  # Last attempt\n",
        "                        raise e\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            for element in soup(['script', 'style', 'nav', 'footer', 'header']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text and clean it\n",
        "            text = ' '.join(soup.stripped_strings)\n",
        "            return text\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error scraping website {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def create_embeddings(self, text: str) -> Tuple[List[str], List[List[float]]]:\n",
        "        \"\"\"Create embeddings from text with progress tracking.\"\"\"\n",
        "        try:\n",
        "            # Split text into chunks\n",
        "            texts = self.text_splitter.split_text(text)\n",
        "            logger.info(f\"Split text into {len(texts)} chunks\")\n",
        "\n",
        "            # Create embeddings with progress bar\n",
        "            vectors = []\n",
        "            for chunk in tqdm(texts, desc=\"Creating embeddings\", leave=False):\n",
        "                try:\n",
        "                    vector = self.embeddings.embed_query(chunk)\n",
        "                    vectors.append(vector)\n",
        "                    time.sleep(0.1)  # Rate limiting\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error creating embedding for chunk: {e}\")\n",
        "                    vectors.append([0] * 1536)  # OpenAI embedding dimension\n",
        "\n",
        "            return texts, vectors\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in embedding creation: {e}\")\n",
        "            return [], []\n",
        "\n",
        "    def process_and_save(self, url: str, output_filename: str = \"website_embeddings.json\") -> None:\n",
        "        \"\"\"Main processing function.\"\"\"\n",
        "        try:\n",
        "            # Scrape website\n",
        "            logger.info(f\"Scraping website: {url}\")\n",
        "            text = self.scrape_website(url)\n",
        "            if not text:\n",
        "                raise ValueError(\"No text retrieved from website\")\n",
        "\n",
        "            # Create embeddings\n",
        "            texts, embeddings = self.create_embeddings(text)\n",
        "            if not texts or not embeddings:\n",
        "                raise ValueError(\"Failed to create embeddings\")\n",
        "\n",
        "            # Prepare output data\n",
        "            output_data = {\n",
        "                \"metadata\": {\n",
        "                    \"url\": url,\n",
        "                    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                    \"num_chunks\": len(texts)\n",
        "                },\n",
        "                \"embeddings\": [\n",
        "                    {\n",
        "                        \"text\": text,\n",
        "                        \"embedding\": embedding\n",
        "                    }\n",
        "                    for text, embedding in zip(texts, embeddings)\n",
        "                ]\n",
        "            }\n",
        "\n",
        "            # Save to file\n",
        "            with open(output_filename, 'w') as f:\n",
        "                json.dump(output_data, f)\n",
        "\n",
        "            # Download file in Colab\n",
        "            files.download(output_filename)\n",
        "\n",
        "            logger.info(f\"Successfully processed website and saved embeddings to {output_filename}\")\n",
        "\n",
        "            # Create FAISS index for similarity search\n",
        "            embedding_dimension = len(embeddings[0])\n",
        "            index = faiss.IndexFlatL2(embedding_dimension)\n",
        "            embeddings_array = np.array(embeddings).astype('float32')\n",
        "            index.add(embeddings_array)\n",
        "\n",
        "            return index, texts  # Return index and texts for immediate use if needed\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in main processing: {e}\")\n",
        "            raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function with user input handling.\"\"\"\n",
        "    try:\n",
        "        # Get user inputs\n",
        "        url = input(\"Enter the URL to scrape: \")\n",
        "        api_key = input(\"Enter your OpenAI API Key: \")\n",
        "\n",
        "        # Create processor and run\n",
        "        processor = WebsiteEmbeddingCreator(api_key)\n",
        "        index, texts = processor.process_and_save(url)\n",
        "\n",
        "        print(\"\\nProcessing complete! You can now use the index for similarity search.\")\n",
        "        print(\"Example usage:\")\n",
        "        print(\"query = 'your search query'\")\n",
        "        print(\"D, I = index.search(processor.embeddings.embed_query(query).reshape(1, -1), k=5)\")\n",
        "        print(\"similar_texts = [texts[i] for i in I[0]]\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in main execution: {e}\")\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "9-RQflNU_Qb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}